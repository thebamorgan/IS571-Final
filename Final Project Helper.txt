IMPORTANT: I provide every student with this helper file to guide you through this individual 
project (take home exam).  To ensure fairness, no other help will be provided to any student.  Please do 
not ask me (or any other student) to explain my sample code. I can  only clarify my instructions
during this final project week.  I do not encourage you to cut and paste my sample code.  In case
that you have to cut and paste some code from this file,please add comments to explain your code line by line.    



===============================================================================
#load the loan data set, relevel loanStatus and DIRatio 
> loan=read.csv("C:/ba/LoanData.csv",stringsAsFactors=T)
> names(loan)
[1] "Status"               "Credit.Grade"         "Amount"               "Age"                 
[5] "Borrower.Rate"        "Debt.To.Income.Ratio"
> levels(loan$Status)
[1] "Current" "Default" "Late"   
> levels(loan$Status)=c(0,1,1)
> loan$Status

> loan$Ratio=cut(loan$Debt.To.Income.Ratio, breaks=c(-1,.1,.3,99999), labels=c("low","medium", "high"))

> logloan=glm(Status~Credit.Grade+Amount+Age+Borrower.Rate+Ratio,family=binomial,data=loan)
> summary(logloan)

#In sample fitting using .5 as the cutoff probability
> pred=predict(logloan,newdata=loan,type="response")
> ttt=table(loan$Status,floor(pred+.5))
> ttt

> table(loan$Status)



#cutoff prbability is 92.43% with symmetric costs of misclassification
> 425/(425+5186)
[1] 0.07574407
> 1-425/(425+5186)
[1] 0.9242559
   
> ttt=table(loan$Status,floor(pred+.9242559))
> ttt
   

#overall in-sample misclassification rate

> (ttt[1,2]+ttt[2,1])/sum(ttt)


#out of sample performance

> train=sample(1:5611,4611)
> logloan2=glm(Status~Credit.Grade+Amount+Age+Borrower.Rate+Ratio,family=binomial,data=loan[train,])
> pred=predict(logloan2,newdata=loan[-train,],type="response")
> table(loan$Status[train])
> cut=table(loan$Status[train])[1]/(table(loan$Status[train])[1]+table(loan$Status[train])[2])
> cut
> ttt=table(loan$Status[-train],floor(pred+cut))
> ttt
#out-of-sample prediction accuracy   
> (ttt[1,2]+ttt[2,1])/sum(ttt)


## order loans in test set according to their default prob
## actual outcome shown next to it
> dd=data.frame(pred,loan$Status[-train])
> dd1=dd[order(pred,decreasing=T),]
> dd1[1:15,]


## overall default probability in evaluation (test) data set
> dslope=mean(as.numeric(loan$Status[-train]))-1
> dslope



## calculating the lift
> x=dim(1000)
> y=dim(1000)
> r=dim(1000)
> x[1]=1
> y[1]=as.numeric(dd1[1,2])-1
> r[1]=dslope
> for (i in 2:1000){
    x[i]=i
    r[i]=i*dslope 
    y[i]=y[i-1]+as.numeric(dd1[i,2])-1}

> plot(x,y,xlab="number of loans",ylab="number of bad loans",main="Lift: Cum prediction successes")
> points(x,r,col="green")



#out of sample accurate rate for 20 random test samples

> accurate=dim(20)
> for (j in 1:20){
      train=sample(1:5611,4611)
      logloan20=glm(Status~Credit.Grade+Amount+Age+Borrower.Rate+Ratio,family=binomial,data=loan[train,])
      pred=predict(logloan20,newdata=loan[-train,],type="response")
      cut=table(loan$Status[train])[1]/(table(loan$Status[train])[1]+table(loan$Status[train])[2])
      ttt=table(loan$Status[-train],floor(pred+cut))
      accurate[j]=(ttt[1,1]+ttt[2,2])/sum(ttt)
  }

> mean(accurate)


===========================================================================


#movie review text classifier using Python NLTK

#Code tested Using Anaconda 2.1 Spyder

import nltk

from nltk.corpus import movie_reviews

import random

movie_reviews.words()

len(movie_reviews.words())

movie_reviews.categories()

len(movie_reviews.fileids())

movie_reviews.raw(movie_reviews.fileids()[10])

movie_reviews.readme()

>>> documents = [(list(movie_reviews.words(fileid)), category)
...              for category in movie_reviews.categories()
...              for fileid in movie_reviews.fileids(category)]

>>> random.shuffle(documents)

#To limit the number of features that the classifier needs to process, we begin by constructing
# a list of the 2000 most frequent words in the overall corpus. We can then define a 
# feature extractor that simply checks whether each of these words is present in a given document.

all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())

#alternatively remove stopwords

from nltk.corpus import stopwords
stopwords = nltk.corpus.stopwords.words('english')
all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words() if w not in stopwords)

import string
all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words() if w not in stopwords if w not in string.punctuation)


all_words['worst']  #dictionary

word_features = sorted(list(all_words.keys()),key=all_words.get,reverse=True)[:2000]


def document_features(document): 
    document_words = set(document) 
    features = {}
    for word in word_features:
        features['contains(%s)' % word] = (word in document_words)
    return features


featuresets = [(document_features(d), c) for (d,c) in documents]

len(featuresets)

train_set, test_set = featuresets[100:], featuresets[:100]

classifier = nltk.NaiveBayesClassifier.train(train_set)

print(nltk.classify.accuracy(classifier, test_set)) 

classifier.show_most_informative_features(15) 

---------------------------------------------------

# Twenty fold cross validation 

accu=0
for i in range(1, 21):
    print(i)
    train_set, test_set = featuresets[:100*(i-1)]+featuresets[100*i:], featuresets[100*(i-1):100*i]
    classifier = nltk.NaiveBayesClassifier.train(train_set)
    arate=nltk.classify.accuracy(classifier, test_set)
    accu=accu+arate
    print('accuracy',arate)

print(accu/i)





